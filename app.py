import streamlit as st
import requests
import json
import jieba  # ğŸŒŸ æ–°å¢ä¸­æ–‡åˆ†è¯
from utils.retriever_pipeline import retrieve_documents
from utils.doc_handler import process_documents
from utils.chinese_tools import chinese_text_preprocess
from sentence_transformers import CrossEncoder
import torch
import os
from dotenv import load_dotenv, find_dotenv
import logging
from logging.handlers import RotatingFileHandler
import sys
import time

# åœ¨æ–‡ä»¶å¤´éƒ¨æ·»åŠ è¯·æ±‚IDç”¨äºè¿½è¸ª
import uuid
current_request_id = str(uuid.uuid4())[:8]

def setup_logging():
    root_logger = logging.getLogger()
    
    # å…³é”®ä¿®å¤ï¼šæ¸…ç†ç°æœ‰handler
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '[%(asctime)s] %(levelname)s @ %(module)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆè‡ªåŠ¨è½®è½¬ï¼‰
    file_handler = RotatingFileHandler(
        'logs/app.log',
        maxBytes=1024*1024*5,  # 5MB
        backupCount=3,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)

    # æ§åˆ¶å°å¤„ç†å™¨
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)

    # é…ç½®æ ¹æ—¥å¿—
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(stream_handler)

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
setup_logging()

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]
load_dotenv(find_dotenv())

OLLAMA_BASE_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_API_URL = f"{OLLAMA_BASE_URL}/api/generate"
MODEL = os.getenv("MODEL", "deepseek-r1:1.5b")  # ğŸŒŸ æ”¹ç”¨ä¸­æ–‡æ¨¡å‹
EMBEDDINGS_MODEL = "moka-ai/m3e-base"  # ğŸŒŸ ä¸­æ–‡åµŒå…¥æ¨¡å‹
CROSS_ENCODER_MODEL = "BAAI/bge-reranker-base"  # ğŸŒŸ ä¸­æ–‡é‡æ’åºæ¨¡å‹

device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸŒŸ åˆå§‹åŒ–ä¸­æ–‡æ¨¡å‹
if "jieba_initialized" not in st.session_state:
    jieba.initialize()
    jieba.load_userdict("data/custom_words.txt")  # è‡ªå®šä¹‰è¯å…¸
    st.session_state.jieba_initialized = True

reranker = None
try:
    # ğŸŒŸ ä½¿ç”¨ä¸­æ–‡é‡æ’åºå™¨
    # åˆå§‹åŒ–æ¨¡å‹
    reranker = CrossEncoder(
        CROSS_ENCODER_MODEL,
        device=device,
        max_length=512
    )
    
except OSError as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)} (è¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¨¡å‹è·¯å¾„)")
except RuntimeError as e:
    if "CUDA out of memory" in str(e):
        st.error("æ˜¾å­˜ä¸è¶³ï¼Œå°è¯•å‡å° batch size æˆ–ä½¿ç”¨ CPU æ¨¡å¼")
    else:
        st.error(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
except Exception as e:
    st.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")

# ğŸŒŸ æ±‰åŒ–ç•Œé¢
st.set_page_config(page_title="æ·±åº¦å›¾è°±æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ", layout="wide")

# ğŸŒŸ ä¸­æ–‡CSSæ ·å¼
st.markdown("""
    <style>
        .stApp { background-color: #f4f4f9; }
        h1 { color: #00FF99; text-align: center; }
        .stChatMessage { border-radius: 10px; padding: 10px; margin: 10px 0; }
        .stChatMessage.user { background-color: #e8f0fe; }
        .stChatMessage.assistant { background-color: #d1e7dd; }
        .stButton>button { background-color: #00AAFF; color: white; }
    </style>
""", unsafe_allow_html=True)

# ğŸŒŸ ä¸­æ–‡ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "retrieval_pipeline" not in st.session_state:
    st.session_state.retrieval_pipeline = None
if "rag_enabled" not in st.session_state:
    st.session_state.rag_enabled = False
if "documents_loaded" not in st.session_state:
    st.session_state.documents_loaded = False

# ğŸŒŸ ä¾§è¾¹æ æ±‰åŒ–
with st.sidebar:
    st.header("ğŸ“ æ–‡æ¡£ç®¡ç†")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒPDF/DOCX/TXTï¼‰",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )
    
    # åœ¨æ–‡æ¡£å¤„ç†éƒ¨åˆ†æ·»åŠ ï¼š
    if uploaded_files and not st.session_state.documents_loaded:
        try:
            logging.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ä¸Šä¼ ï¼šæ”¶åˆ°{len(uploaded_files)}ä¸ªæ–‡ä»¶")
            with st.spinner("æ–‡æ¡£å¤„ç†ä¸­..."):
                process_documents(uploaded_files, reranker, EMBEDDINGS_MODEL, device)
                logging.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œæ–‡ä»¶åï¼š{[f.name for f in uploaded_files]}")
                st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        except Exception as e:
            logging.error("æ–‡æ¡£å¤„ç†å¤±è´¥", exc_info=True)
            st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    
    st.markdown("---")
    st.header("âš™ï¸ æ£€ç´¢è®¾ç½®")
    
    st.session_state.rag_enabled = st.checkbox("å¯ç”¨æ™ºèƒ½æ£€ç´¢", value=True)
    st.session_state.enable_hyde = st.checkbox("å¯ç”¨æŸ¥è¯¢æ‰©å±•", value=False)
    st.session_state.enable_reranking = st.checkbox("å¯ç”¨ç¥ç»é‡æ’åº", value=False)
    st.session_state.enable_graph_rag = st.checkbox("å¯ç”¨çŸ¥è¯†å›¾è°±", value=False)
    st.session_state.temperature = st.slider("ç”Ÿæˆæ¸©åº¦", 0.0, 1.0, 0.3, 0.05)
    st.session_state.max_contexts = st.slider("æœ€å¤§ä¸Šä¸‹æ–‡", 1, 5, 3)
    
    if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("""
        <div style="font-size: 12px; color: gray;">
            <b>å¼€å‘è€…ï¼š</b>wangenyong &copy; ç‰ˆæƒæ‰€æœ‰ 2025
        </div>
    """, unsafe_allow_html=True)

# ğŸŒŸ ä¸»ç•Œé¢æ±‰åŒ–
st.title("ğŸ¤– æ·±åº¦å›¾è°±æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ")
st.caption("é›†æˆçŸ¥è¯†å›¾è°±ã€æ··åˆæ£€ç´¢ä¸ç¥ç»é‡æ’åºçš„å…ˆè¿›é—®ç­”ç³»ç»Ÿ")


# å¯¹è¯æ˜¾ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # ğŸŒŸ ä¸­æ–‡é¢„å¤„ç†
    processed_prompt = chinese_text_preprocess(prompt)
    
    chat_history = "\n".join([msg["content"] for msg in st.session_state.messages[-5:]])
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        
        # ğŸŒŸ æ–°å¢åŠ è½½åŠ¨ç”»ç»„ä»¶
        with response_placeholder.container():
            st.markdown("""
            <div style="display: flex; align-items: center; gap: 0.8rem; color: #4a4a4a; position: relative; top: -6px;">
                <div class="loader"></div>
                <div>æ­£åœ¨æ€è€ƒä¸­...</div>
            </div>
            <style>
            .loader {
                border: 3px solid #f3f3f3;
                border-radius: 50%;
                border-top: 3px solid #409EFF;
                width: 24px;
                height: 24px;
                animation: spin 1s linear infinite;
            }
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            </style>
            """, unsafe_allow_html=True)
        
        full_response = ""
        
        # ğŸŒŸ ä¸­æ–‡ä¼˜åŒ–ä¸Šä¸‹æ–‡æ„å»º
        context = ""
        # åœ¨æ£€ç´¢è¿‡ç¨‹æ·»åŠ æ—¥å¿—ï¼š
        if st.session_state.rag_enabled and st.session_state.retrieval_pipeline:
            try:
                logging.info(f"å¼€å§‹æ–‡æ¡£æ£€ç´¢ | æŸ¥è¯¢ï¼š{processed_prompt}")
                docs = retrieve_documents(processed_prompt, OLLAMA_API_URL, MODEL, chat_history)
                logging.info(f"æ£€ç´¢å®Œæˆ | è·å¾—{docs and len(docs) or 0}æ¡ç›¸å…³æ–‡æ¡£")
                context = "\n".join(
                    f"[æ¥æº {i+1}]: {doc.page_content}" 
                    for i, doc in enumerate(docs)
                )
            except Exception as e:
                logging.error("æ–‡æ¡£æ£€ç´¢å¤±è´¥", exc_info=True)
                st.error(f"æ£€ç´¢é”™è¯¯: {str(e)}")
        
        try:
            # ğŸŒŸ åˆå§‹åŒ–å…³é”®å˜é‡
            full_response = ""
            response_buffer = b""
            token_count = 0
            start_time = time.time()
            
            logging.info(f"[{current_request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ | æ¸©åº¦={st.session_state.temperature} | æœ€å¤§ä¸Šä¸‹æ–‡={st.session_state.max_contexts}")
            
            # ğŸŒŸ å¢å¼ºæç¤ºè¯ç»“æ„
            system_prompt = f"""åŸºäºæœ¬åœ°çŸ¥è¯†åº“ç”¨ä¸­æ–‡ä¸“ä¸šåœ°å›ç­”ï¼Œä¸¥æ ¼éµå¾ªæ­¥éª¤ï¼š
            ğŸ“šã€çŸ¥è¯†åº“ä½¿ç”¨åŸåˆ™ã€‘
            1. ä¼˜å…ˆä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“å†…å®¹ï¼ˆå…±{len(docs)}æ¡ï¼‰ï¼š
            {context[:1500]}...
            2. è‹¥çŸ¥è¯†åº“å†…å®¹ä¸è¶³ï¼Œéœ€æ˜ç¡®è¯´æ˜ã€Œæ ¹æ®ç°æœ‰çŸ¥è¯†åº“ä¿¡æ¯ã€å¹¶ç»™å‡ºå»ºè®®æ€§å›ç­”
            3. å½“ä¸åŒæ¥æºå†²çªæ—¶ï¼Œæ ‡æ³¨å·®å¼‚å¹¶å»ºè®®æ ¸å®

            ğŸ”ã€å›ç­”æ­¥éª¤ã€‘
            1. å®ä½“æå–ï¼šè¯†åˆ«é—®é¢˜ä¸­çš„å…³é”®å®ä½“ï¼ˆæ ‡è“æ˜¾ç¤ºï¼‰
            2. æ¥æºåˆ†æï¼šå¯¹æ¯ä¸ªçŸ¥è¯†ç‰‡æ®µè¿›è¡Œï¼š
            - å¯ä¿¡åº¦è¯„ä¼°ï¼ˆâ˜…â˜†â˜†â˜†ï½â˜…â˜…â˜…â˜…â˜…ï¼‰
            - ä¸é—®é¢˜çš„å…³è”åº¦è¯´æ˜
            3. å·®å¼‚å¤„ç†ï¼š
            {'''- å¯¹æ¯”[æ¥æºX]ä¸[æ¥æºY]åœ¨ã€Œå·®å¼‚ç‚¹ã€ä¸Šçš„è¡¨è¿°''' if len(docs)>1 else '- å•ä¸€æ¥æºæ— éœ€å¯¹æ¯”'}
            4. çŸ¥è¯†ç¼ºå£ï¼š{'' if len(docs)>=3 else 'âš ï¸ æ³¨æ„ï¼šå½“å‰çŸ¥è¯†åº“è¦†ç›–ä¸è¶³'}

            ğŸ“ã€å›ç­”æ ¼å¼ã€‘
            ã€çŸ¥è¯†åˆ†æã€‘ç”¨emojiå›¾æ ‡åŒºåˆ†é˜¶æ®µ
            ğŸ•µï¸ æå–ç»“æœï¼šè“è‰²æ ‡è®°å…³é”®å®ä½“
            ğŸ“Š æ¥æºè¯„ä¼°ï¼šè¡¨æ ¼å±•ç¤º(æ¥æº|å…³é”®ç‚¹|å¯ä¿¡åº¦)
            ğŸ” å·®å¼‚æŠ¥å‘Šï¼š{len(docs)>1 and 'å¯¹æ¯”è¡¨æ ¼' or 'æ— '}
            ã€æœ€ç»ˆç­”æ¡ˆã€‘
            âœ… ç¡®å®šæ€§å›ç­”ï¼ˆå½“çŸ¥è¯†åº“å……è¶³æ—¶ï¼‰
            â“ æ¨æµ‹æ€§å›ç­”ï¼ˆéœ€æ ‡æ³¨ä¸ç¡®å®šæ€§éƒ¨åˆ†ï¼‰

            ç¤ºä¾‹ï¼š
            ã€çŸ¥è¯†åˆ†æã€‘
            ğŸ•µï¸ å…³é”®å®ä½“ï¼šé‡å­è®¡ç®—
            ğŸ“Š æ¥æºè¯„ä¼°ï¼š
            | æ¥æº | å…³é”®ä¿¡æ¯ | å¯ä¿¡åº¦ |
            |------|---------|-------|
            | #1   | ...     | â˜…â˜…â˜…â˜†â˜† |
            ğŸ” å·®å¼‚æŠ¥å‘Šï¼šæ¥æº#1ä¸#2åœ¨é‡å­æ¯”ç‰¹æ•°ä¸Šè¡¨è¿°ä¸åŒï¼ˆ50 vs 53ï¼‰
            ã€æœ€ç»ˆç­”æ¡ˆã€‘
            æ ¹æ®çŸ¥è¯†åº“#1æœ€æ–°ä¿¡æ¯ï¼Œé‡å­è®¡ç®—èŠ¯ç‰‡...ï¼ˆå·®å¼‚éƒ¨åˆ†å»ºè®®æŸ¥é˜…2024æŠ€æœ¯ç™½çš®ä¹¦ï¼‰"""
                
            logging.info(f"[{current_request_id}] å®Œæ•´æç¤ºè¯:\n{system_prompt}")

            # ğŸŒŸ å¢å¼ºè¯·æ±‚è¶…æ—¶è®¾ç½®
            response = requests.post(
                OLLAMA_API_URL,
                json={
                    "model": MODEL,
                    "prompt": system_prompt,
                    "stream": True,
                    "options": {
                        "temperature": max(0.1, min(st.session_state.temperature, 1.0)),  # æ¸©åº¦å€¼å®‰å…¨é™åˆ¶
                        "num_ctx": 4096,
                        "stop": ["\n\n\n", "<|endoftext|>"] 
                    }
                },
                stream=True
            )
            logging.info(f"[{current_request_id}] APIè¯·æ±‚æˆåŠŸ | çŠ¶æ€ç : {response.status_code}")
            
            # ğŸŒŸ æ¸…ç©ºåŠ è½½åŠ¨ç”»
            response_placeholder.empty()  # è¿™é‡Œæ¸…é™¤ä¹‹å‰çš„åŠ è½½åŠ¨ç”»
            
            # ğŸŒŸ æ”¹è¿›æ®µä»£ç å¯¹ ollama deepseek7bè¯·æ±‚å›ç­”ï¼Œä½†æ˜¯think é˜¶æ®µå®Œæˆä¸€å¥è¯ä¹‹åå°±ç»“æŸï¼Œæ²¡æœ‰æ­£å¼çš„å›ç­”ä¿¡æ¯ï¼Œæœ€åä¸€å¥æ•°æ®å¦‚ä¸‹çš„æµå¼å¤„ç†
            for raw_chunk in response.iter_content(chunk_size=512):
                if raw_chunk:
                    response_buffer += raw_chunk
                    # å¤„ç†åˆ†å—å¯èƒ½åŒ…å«å¤šä¸ªJSONçš„æƒ…å†µ
                    while b'\n' in response_buffer:
                        line, response_buffer = response_buffer.split(b'\n', 1)
                        if not line:
                            continue
                        # è®°å½•åŸå§‹æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
                        line_debug = line.decode('utf-8', errors='replace')
                        logging.debug(f"åŸå§‹æ•°æ®: {line_debug}")
                        try:
                            data = json.loads(line.decode('utf-8'))
                            # å¤šå­—æ®µå…¼å®¹
                            token = data.get("response") or data.get("content") or data.get("text") or ""
                            
                            if token:
                                token_count += 1
                                full_response += token
                                # æµå¼æ›´æ–°é¢‘ç‡æ§åˆ¶ï¼ˆæ¯3ä¸ªtokenæˆ–0.5ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
                                if token_count % 3 == 0 or (time.time() - start_time) > 0.5:
                                    response_placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
                                    start_time = time.time()
                            
                            # ç»“æŸæ¡ä»¶åˆ¤æ–­
                            if data.get("done", False):
                                if token := data.get("final_answer"):  # å¦‚æœæœ‰æœ€ç»ˆç­”æ¡ˆå­—æ®µ
                                    full_response += token
                                logging.debug(f"[{current_request_id}] æ”¶åˆ°ç»“æŸæ ‡è®° | æœ€åæ•°æ®: {data}")
                                break
                                
                        except json.JSONDecodeError as e:
                            logging.warning(f"[{current_request_id}] JSONè§£æå¼‚å¸¸ | æ•°æ®å—: {line} | é”™è¯¯: {str(e)}")
                        except Exception as e:
                            logging.error(f"[{current_request_id}] æµå¤„ç†å¼‚å¸¸ | ç±»å‹: {type(e).__name__} | é”™è¯¯: {str(e)} | åŸå§‹æ•°æ®: {line}", exc_info=True)

        except StopIteration:
            logging.info(f"[{current_request_id}] æµå¼å“åº”æ­£å¸¸ç»“æŸ")
        except requests.exceptions.Timeout:
            logging.error(f"[{current_request_id}] è¯·æ±‚è¶…æ—¶ | å·²æ¥æ”¶æ•°æ®: {len(full_response)}å­—")
            st.error("å“åº”è¶…æ—¶ï¼Œè¯·ç®€åŒ–é—®é¢˜é‡è¯•")
        except Exception as e:
            logging.critical(f"[{current_request_id}] æœªæ•è·å¼‚å¸¸", exc_info=True)
            st.error("ç³»ç»Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
        finally:
            # ğŸŒŸ æœ€ç»ˆå¤„ç†
            if full_response:
                response_placeholder.markdown(full_response, unsafe_allow_html=True)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "request_id": current_request_id  # ç”¨äºåç»­è¿½è¸ª
                })
                logging.info(f"[{current_request_id}] å“åº”å®Œæˆ | æ€»è€—æ—¶: {time.time()-start_time:.2f}s | æ€»tokenæ•°: {token_count}")
            else:
                st.error("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå“åº”")
                logging.warning(f"[{current_request_id}] ç©ºå“åº” | ç¼“å†²åŒºå¤§å°: {len(response_buffer)}")
