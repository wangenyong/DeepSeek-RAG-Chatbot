import streamlit as st
import requests
import json
import jieba  # ğŸŒŸ æ–°å¢ä¸­æ–‡åˆ†è¯
from utils.retriever_pipeline import retrieve_documents
from utils.doc_handler import process_documents
from utils.style_files import thinking_style, thinking_loading_style, app_style
from utils.log_tools import setup_logging
from utils.structured_query import is_structured_query
from utils.db_agent import DBAgent
from sentence_transformers import CrossEncoder
import torch
import os
from dotenv import load_dotenv, find_dotenv
import logging
import time

# åœ¨æ–‡ä»¶å¤´éƒ¨æ·»åŠ è¯·æ±‚IDç”¨äºè¿½è¸ª
import uuid
current_request_id = str(uuid.uuid4())[:8]

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
setup_logging()

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]
load_dotenv(find_dotenv())

OLLAMA_BASE_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_API_URL = f"{OLLAMA_BASE_URL}/api/generate"
MODEL = os.getenv("MODEL", "deepseek-r1:1.5b")  # ğŸŒŸ æ”¹ç”¨ä¸­æ–‡æ¨¡å‹
EMBEDDINGS_MODEL = "moka-ai/m3e-base"  # ğŸŒŸ ä¸­æ–‡åµŒå…¥æ¨¡å‹
CROSS_ENCODER_MODEL = "BAAI/bge-reranker-base"  # ğŸŒŸ ä¸­æ–‡é‡æ’åºæ¨¡å‹

device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸŒŸ åˆå§‹åŒ–ä¸­æ–‡æ¨¡å‹
if "jieba_initialized" not in st.session_state:
    jieba.initialize()
    jieba.load_userdict("data/custom_words.txt")  # è‡ªå®šä¹‰è¯å…¸
    st.session_state.jieba_initialized = True

reranker = None
try:
    # ğŸŒŸ ä½¿ç”¨ä¸­æ–‡é‡æ’åºå™¨
    # åˆå§‹åŒ–æ¨¡å‹
    reranker = CrossEncoder(
        CROSS_ENCODER_MODEL,
        device=device,
        max_length=512
    )
    
except OSError as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)} (è¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¨¡å‹è·¯å¾„)")
except RuntimeError as e:
    if "CUDA out of memory" in str(e):
        st.error("æ˜¾å­˜ä¸è¶³ï¼Œå°è¯•å‡å° batch size æˆ–ä½¿ç”¨ CPU æ¨¡å¼")
    else:
        st.error(f"è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
except Exception as e:
    st.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")

# ğŸŒŸ æ±‰åŒ–ç•Œé¢
st.set_page_config(page_title="PEACOCKæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ", layout="wide")

# åœ¨è„šæœ¬æœ€å‰é¢æ·»åŠ æ ·å¼
st.markdown(app_style, unsafe_allow_html=True)

st.title("ğŸ¤– PEACOCKæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ")

# ğŸŒŸ ä¸­æ–‡ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "retrieval_pipeline" not in st.session_state:
    st.session_state.retrieval_pipeline = None
if "rag_enabled" not in st.session_state:
    st.session_state.rag_enabled = False
if "documents_loaded" not in st.session_state:
    st.session_state.documents_loaded = False

# ğŸŒŸ ä¾§è¾¹æ æ±‰åŒ–
with st.sidebar:
    st.header("ğŸ“ æ–‡æ¡£ç®¡ç†")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒPDF/DOCX/TXT/EXCELï¼‰",
        type=["pdf", "docx", "txt", "xls", "xlsx"],
        accept_multiple_files=True
    )
    
    # åœ¨æ–‡æ¡£å¤„ç†éƒ¨åˆ†æ·»åŠ ï¼š
    if uploaded_files and not st.session_state.documents_loaded:
        try:
            logging.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ä¸Šä¼ ï¼šæ”¶åˆ°{len(uploaded_files)}ä¸ªæ–‡ä»¶")
            with st.spinner("æ–‡æ¡£å¤„ç†ä¸­..."):
                process_documents(uploaded_files, reranker, EMBEDDINGS_MODEL, device)
                logging.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œæ–‡ä»¶åï¼š{[f.name for f in uploaded_files]}")
                st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        except Exception as e:
            logging.error("æ–‡æ¡£å¤„ç†å¤±è´¥", exc_info=True)
            st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    
    st.markdown("---")
    st.header("âš™ï¸ æ£€ç´¢è®¾ç½®")
    
    st.session_state.rag_enabled = st.checkbox("å¯ç”¨æ™ºèƒ½æ£€ç´¢", value=True)
    st.session_state.enable_hyde = st.checkbox("å¯ç”¨æŸ¥è¯¢æ‰©å±•", value=False)
    st.session_state.enable_reranking = st.checkbox("å¯ç”¨ç¥ç»é‡æ’åº", value=True)
    #st.session_state.enable_graph_rag = st.checkbox("å¯ç”¨çŸ¥è¯†å›¾è°±", value=False)
    st.session_state.enable_graph_rag = False  # ğŸŒŸ æš‚æ—¶å…³é—­å›¾è°±å¢å¼º
    st.session_state.temperature = st.slider("ç”Ÿæˆæ¸©åº¦", 0.0, 1.0, 0.3, 0.05)
    st.session_state.max_contexts = st.slider("æœ€å¤§ä¸Šä¸‹æ–‡", 1, 5, 3)
    
    if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("""
        <div style="font-size: 12px; color: gray;">
            <b>å¼€å‘è€…ï¼š</b>wangenyong &copy; ç‰ˆæƒæ‰€æœ‰ 2025
        </div>
    """, unsafe_allow_html=True)
    

# å¯¹è¯æ˜¾ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    chat_history = "\n".join([msg["content"] for msg in st.session_state.messages[-5:]])
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        think_placeholder = st.empty()
        response_placeholder = st.empty()
        
        # ğŸŒŸ æ–°å¢åŠ è½½åŠ¨ç”»ç»„ä»¶
        with think_placeholder.container():
            st.markdown(thinking_loading_style, unsafe_allow_html=True)
        
        think_response = ""
        full_response = ""
        
        # ğŸŒŸ ä¸­æ–‡ä¼˜åŒ–ä¸Šä¸‹æ–‡æ„å»º
        context = ""
        if is_structured_query(prompt):
            db_agent = DBAgent()
            db_result = db_agent.query(prompt)
            context += f"\næ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š{db_result['summary'] if db_result else 'æ— ç›¸å…³æ•°æ®'}"
        elif st.session_state.rag_enabled and st.session_state.retrieval_pipeline:
            try:
                logging.info(f"å¼€å§‹æ–‡æ¡£æ£€ç´¢ | æŸ¥è¯¢ï¼š{prompt}")
                docs = retrieve_documents(prompt, OLLAMA_API_URL, MODEL, chat_history)
                logging.info(f"æ£€ç´¢å®Œæˆ | è·å¾—{docs and len(docs) or 0}æ¡ç›¸å…³æ–‡æ¡£")
                context = "\n".join(
                    f"[æ¥æº {i+1}]: {doc.page_content}" 
                    for i, doc in enumerate(docs)
                )
            except Exception as e:
                logging.error("æ–‡æ¡£æ£€ç´¢å¤±è´¥", exc_info=True)
                st.error(f"æ£€ç´¢é”™è¯¯: {str(e)}")
        
        try:
            # ğŸŒŸ åˆå§‹åŒ–å…³é”®å˜é‡
            think_response = ""
            full_response = ""
            response_buffer = b""
            token_count = 0
            start_time = time.time()
            
            logging.info(f"[{current_request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ | æ¸©åº¦={st.session_state.temperature} | æœ€å¤§ä¸Šä¸‹æ–‡={st.session_state.max_contexts}")
            
            # ğŸŒŸ å¢å¼ºæç¤ºè¯ç»“æ„
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰å’Œç”¨æˆ·çš„é—®é¢˜ï¼ˆQueryï¼‰ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆä¸€ä¸ªæ¸…æ™°ä¸”å‡†ç¡®çš„å›ç­”ã€‚éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
                1. ä»…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸ç›¸å…³æˆ–æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”æ­¤é—®é¢˜â€ã€‚
                2. é¿å…ç¼–é€ ä¿¡æ¯æˆ–å‡è®¾æœªçŸ¥ç»†èŠ‚ã€‚
                3. å¦‚æœå›ç­”éœ€è¦æ­¥éª¤ã€åˆ—è¡¨æˆ–ä»£ç ï¼Œè¯·ä½¿ç”¨ Markdown æ ¼å¼ã€‚

                ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰ï¼š
                {context}

                ç”¨æˆ·é—®é¢˜ï¼ˆQueryï¼‰ï¼š
                {prompt}

                å›ç­”ï¼š"""

            # ğŸŒŸ å¢å¼ºè¯·æ±‚è¶…æ—¶è®¾ç½®
            response = requests.post(
                OLLAMA_API_URL,
                json={
                    "model": MODEL,
                    "prompt": system_prompt,
                    "stream": True,
                    "options": {
                        "temperature": max(0.1, min(st.session_state.temperature, 1.0)),  # æ¸©åº¦å€¼å®‰å…¨é™åˆ¶
                        "num_ctx": 4096,
                        "stop": ["\n\n\n", "<|endoftext|>"] 
                    }
                },
                stream=True
            )
            logging.info(f"[{current_request_id}] APIè¯·æ±‚æˆåŠŸ | çŠ¶æ€ç : {response.status_code}")
            
            # ğŸŒŸ æ¸…ç©ºåŠ è½½åŠ¨ç”»
            think_placeholder.empty()  # è¿™é‡Œæ¸…é™¤ä¹‹å‰çš„åŠ è½½åŠ¨ç”»
            
            think_mode = False
            
            for raw_chunk in response.iter_content(chunk_size=512):
                if raw_chunk:
                    response_buffer += raw_chunk
                    # å¤„ç†åˆ†å—å¯èƒ½åŒ…å«å¤šä¸ªJSONçš„æƒ…å†µ
                    while b'\n' in response_buffer:
                        line, response_buffer = response_buffer.split(b'\n', 1)
                        if not line:
                            continue
                        # è®°å½•åŸå§‹æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰
                        line_debug = line.decode('utf-8', errors='replace')
                        logging.debug(f"åŸå§‹æ•°æ®: {line_debug}")
                        try:
                            data = json.loads(line.decode('utf-8'))
                            # å¤šå­—æ®µå…¼å®¹
                            token = data.get("response") or data.get("content") or data.get("text") or ""
                            
                            if token:
                                token_count += 1
                                if think_mode == False and "<think>" in token:
                                    logging.info(f"[{current_request_id}] å‘ç°æ€è€ƒæ ‡è®° | æ•°æ®: {token}")
                                    think_mode = True
                                    think_response += token
                                elif think_mode == True and "</think>" in token:
                                    logging.info(f"[{current_request_id}] ç»“æŸæ€è€ƒæ ‡è®° | æ•°æ®: {token}")
                                    think_mode = False
                                    think_response += token
                                elif think_mode == True:
                                    think_response += token
                                else:
                                    full_response += token
                        
                                # æµå¼æ›´æ–°é¢‘ç‡æ§åˆ¶ï¼ˆæ¯3ä¸ªtokenæˆ–0.5ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
                                if token_count % 3 == 0 or (time.time() - start_time) > 0.5:
                                    if think_mode:
                                        think_placeholder.markdown(thinking_style.format(think_response + "â–Œ"), unsafe_allow_html=True)
                                    else:
                                        response_placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
                                    start_time = time.time()
                            
                            # ç»“æŸæ¡ä»¶åˆ¤æ–­
                            if data.get("done", False):
                                if token := data.get("final_answer"):  # å¦‚æœæœ‰æœ€ç»ˆç­”æ¡ˆå­—æ®µ
                                    full_response += token
                                logging.debug(f"[{current_request_id}] æ”¶åˆ°ç»“æŸæ ‡è®° | æœ€åæ•°æ®: {data}")
                                break
                                
                        except json.JSONDecodeError as e:
                            logging.warning(f"[{current_request_id}] JSONè§£æå¼‚å¸¸ | æ•°æ®å—: {line} | é”™è¯¯: {str(e)}")
                        except Exception as e:
                            logging.error(f"[{current_request_id}] æµå¤„ç†å¼‚å¸¸ | ç±»å‹: {type(e).__name__} | é”™è¯¯: {str(e)} | åŸå§‹æ•°æ®: {line}", exc_info=True)

        except StopIteration:
            logging.info(f"[{current_request_id}] æµå¼å“åº”æ­£å¸¸ç»“æŸ")
        except requests.exceptions.Timeout:
            logging.error(f"[{current_request_id}] è¯·æ±‚è¶…æ—¶ | å·²æ¥æ”¶æ•°æ®: {len(full_response)}å­—")
            st.error("å“åº”è¶…æ—¶ï¼Œè¯·ç®€åŒ–é—®é¢˜é‡è¯•")
        except Exception as e:
            logging.critical(f"[{current_request_id}] æœªæ•è·å¼‚å¸¸", exc_info=True)
            st.error("ç³»ç»Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
        finally:
            # ğŸŒŸ æœ€ç»ˆå¤„ç†
            if full_response:
                think_placeholder.markdown(thinking_style.format(think_response), unsafe_allow_html=True)
                response_placeholder.markdown(full_response, unsafe_allow_html=True)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "request_id": current_request_id  # ç”¨äºåç»­è¿½è¸ª
                })
                logging.info(f"[{current_request_id}] å“åº”å®Œæˆ | æ€»è€—æ—¶: {time.time()-start_time:.2f}s | æ€»tokenæ•°: {token_count}")
            else:
                st.error("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå“åº”")
                logging.warning(f"[{current_request_id}] ç©ºå“åº” | ç¼“å†²åŒºå¤§å°: {len(response_buffer)}")
