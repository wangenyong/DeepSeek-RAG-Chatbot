import streamlit as st
import networkx as nx
import jieba
import jieba.posseg as pseg
from text2vec import Similarity

# åŠ è½½ä¸­æ–‡NLPèµ„æº
jieba.initialize()
SIM_MODEL = Similarity()

def build_knowledge_graph(docs):
    """ä¼˜åŒ–åçš„ä¸­æ–‡çŸ¥è¯†å›¾è°±æ„å»º"""
    G = nx.Graph()
    
    for doc in docs:
        # ä½¿ç”¨è¯æ€§æ ‡æ³¨æå–å®ä½“
        words = pseg.cut(doc.page_content)
        entities = [
            word.word for word in words 
            if word.flag in ['nr', 'ns', 'nt', 'nz']  # äººå/åœ°å/æœºæ„å/å…¶ä»–ä¸“å
        ]
        
        # æ·»åŠ è¯­ä¹‰å…³ç³»ï¼ˆä½¿ç”¨Text2Vecè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        for i in range(len(entities)):
            for j in range(i+1, len(entities)):
                score = SIM_MODEL.get_score(entities[i], entities[j])  # è¡¥ä¸Šscoreå®šä¹‰
                if score > 0.6:
                    G.add_edge(entities[i], entities[j], weight=score)  # ä½¿ç”¨å®é™…è®¡ç®—å€¼
    
    return G

def retrieve_from_graph(query, G, top_k=5):
    """ä¼˜åŒ–åçš„ä¸­æ–‡å›¾è°±æ£€ç´¢"""
    st.write(f"ğŸ” çŸ¥è¯†å›¾è°±æ£€ç´¢: {query}")
    
    # ä¸­æ–‡æŸ¥è¯¢è§£æ
    query_entities = [
        word.word for word in pseg.cut(query)
        if word.flag in ['nr', 'ns', 'nt', 'nz']
    ]
    
    if not query_entities:
        return []

    # è¯­ä¹‰æ‰©å±•æ£€ç´¢
    results = []
    for node in G.nodes:
        sim_score = max(SIM_MODEL.get_score(node, ent) for ent in query_entities)
        if sim_score > 0.5:
            results.append((node, sim_score))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results = sorted(results, key=lambda x: -x[1])[:top_k]
    
    if results:
        st.write(f"ğŸŸ¢ åŒ¹é…å®ä½“: {[x[0] for x in results]}")
        return [x[0] for x in results]
    
    return []

