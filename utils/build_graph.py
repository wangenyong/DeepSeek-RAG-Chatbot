import streamlit as st
import networkx as nx
import jieba
import jieba.posseg as pseg
from text2vec import Similarity
import logging
from datetime import datetime

# åŠ è½½ä¸­æ–‡NLPèµ„æº
jieba.initialize()
SIM_MODEL = Similarity()

def build_knowledge_graph(docs):
    """ä¼˜åŒ–åçš„ä¸­æ–‡çŸ¥è¯†å›¾è°±æ„å»º"""
    G = nx.Graph()
    logging.info("ğŸ—ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°± | è¾“å…¥æ–‡æ¡£æ•°ï¼š%d", len(docs))
    
    total_entities = 0
    total_relations = 0
    start_time = datetime.now()
    
    try:
        for idx, doc in enumerate(docs):
            # å®ä½“æå–
            try:
                words = pseg.cut(doc.page_content)
                entities = [
                    word.word for word in words 
                    if word.flag in ['nr', 'ns', 'nt', 'nz']
                ]
                logging.debug("æ–‡æ¡£[%d/%d] å®ä½“æå– | åŸå§‹æ–‡æœ¬é•¿åº¦ï¼š%d | æå–å®ä½“æ•°ï¼š%d",
                             idx+1, len(docs), len(doc.page_content), len(entities))
                total_entities += len(entities)
                
                if not entities:
                    logging.debug("æ–‡æ¡£[%d/%d] æœªæ£€æµ‹åˆ°æœ‰æ•ˆå®ä½“", idx+1, len(docs))
                    continue

                # è®°å½•ç¤ºä¾‹å®ä½“ï¼ˆé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼‰
                sample_entities = [e[:min(5, len(e))]+"..." if len(e)>5 else e for e in entities[:3]]
                logging.debug("æ–‡æ¡£[%d/%d] ç¤ºä¾‹å®ä½“ï¼š%s", idx+1, len(docs), sample_entities)

            except Exception as e:
                logging.error("æ–‡æ¡£[%d/%d] å®ä½“æå–å¤±è´¥", idx+1, len(docs), exc_info=True)
                continue

            # å…³ç³»æ„å»º
            relation_count = 0
            for i in range(len(entities)):
                for j in range(i+1, len(entities)):
                    try:
                        score = SIM_MODEL.get_score(entities[i], entities[j])
                        if score > 0.6:
                            G.add_edge(entities[i], entities[j], weight=score)
                            relation_count += 1
                            total_relations += 1
                            
                            # è®°å½•é«˜åˆ†å…³ç³»
                            if score > 0.8:
                                logging.debug("å‘ç°å¼ºå…³è” | %s â†” %s | åˆ†æ•°ï¼š%.2f",
                                            entities[i], entities[j], score)
                    except Exception as e:
                        logging.error("ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ | å®ä½“å¯¹ï¼š%s-%s",
                                     entities[i], entities[j], exc_info=True)

            logging.debug("æ–‡æ¡£[%d/%d] æ·»åŠ å…³ç³»æ•°ï¼š%d", idx+1, len(docs), relation_count)

        duration = (datetime.now() - start_time).total_seconds()
        logging.info("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ | æ€»è€—æ—¶ï¼š%.2fs | æ€»å®ä½“æ•°ï¼š%d | æ€»å…³ç³»æ•°ï¼š%d",
                    duration, total_entities, total_relations)
        logging.debug("å›¾è°±ç»Ÿè®¡ | èŠ‚ç‚¹åº¦ç¤ºä¾‹ï¼š%s", 
                     dict(list(G.degree())[:5]))  # æ˜¾ç¤ºå‰5ä¸ªèŠ‚ç‚¹çš„åº¦

    except Exception as e:
        logging.error("çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹å¼‚å¸¸ç»ˆæ­¢", exc_info=True)
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†

    return G

def retrieve_from_graph(query, G, top_k=5):
    """ä¼˜åŒ–åçš„ä¸­æ–‡å›¾è°±æ£€ç´¢"""
    logging.info("ğŸ” å¼€å§‹å›¾è°±æ£€ç´¢ | æŸ¥è¯¢ï¼š%s | æœ€å¤§ç»“æœæ•°ï¼š%d", query, top_k)
    start_time = datetime.now()
    
    try:
        # å®ä½“æå–
        query_entities = [
            word.word for word in pseg.cut(query)
            if word.flag in ['nr', 'ns', 'nt', 'nz']
        ]
        logging.info("æŸ¥è¯¢è§£æ | æå–å®ä½“æ•°ï¼š%d | å®ä½“åˆ—è¡¨ï¼š%s", 
                    len(query_entities), query_entities[:5])  # é™åˆ¶æ˜¾ç¤ºæ•°é‡

        if not query_entities:
            logging.warning("æŸ¥è¯¢æœªåŒ…å«å¯è¯†åˆ«å®ä½“ | æŸ¥è¯¢å†…å®¹ï¼š%s", query[:50]+"..." if len(query)>50 else query)
            return []

        # è¯­ä¹‰æ‰©å±•æ£€ç´¢
        results = []
        logging.debug("å¼€å§‹èŠ‚ç‚¹éå† | æ€»èŠ‚ç‚¹æ•°ï¼š%d", len(G.nodes))
        for node in G.nodes:
            try:
                sim_score = max(SIM_MODEL.get_score(node, ent) for ent in query_entities)
                if sim_score > 0.5:
                    results.append((node, sim_score))
                    logging.debug("èŠ‚ç‚¹åŒ¹é… | %s â†” %s | æœ€é«˜åˆ†ï¼š%.2f",
                                 node, query_entities, sim_score)
            except Exception as e:
                logging.error("èŠ‚ç‚¹ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ | èŠ‚ç‚¹ï¼š%s", node, exc_info=True)

        # ç»“æœæ’åº
        sorted_results = sorted(results, key=lambda x: -x[1])[:top_k]
        logging.info("æ£€ç´¢å®Œæˆ | å€™é€‰ç»“æœæ•°ï¼š%d | æœ€é«˜åˆ†ï¼š%.2f", 
                    len(sorted_results), sorted_results[0][1] if sorted_results else 0)

        if sorted_results:
            logging.debug("TOPç»“æœç¤ºä¾‹ï¼š%s", ["%s(%.2f)"%(x[0],x[1]) for x in sorted_results[:3]])
            st.write(f"ğŸŸ¢ åŒ¹é…å®ä½“: {[x[0] for x in sorted_results]}")
        else:
            logging.info("æ— ç¬¦åˆé˜ˆå€¼çš„ç»“æœ")

        duration = (datetime.now() - start_time).total_seconds()
        logging.info("å›¾è°±æ£€ç´¢å®Œæˆ | æ€»è€—æ—¶ï¼š%.2fs", duration)
        
        return [x[0] for x in sorted_results]

    except Exception as e:
        logging.error("å›¾è°±æ£€ç´¢æµç¨‹å¼‚å¸¸ç»ˆæ­¢", exc_info=True)
        raise


