import streamlit as st
from utils.build_graph import retrieve_from_graph
from langchain_core.documents import Document
import requests
import jieba  # ğŸŒŸ æ–°å¢ä¸­æ–‡åˆ†è¯åº“
import re

# ğŸŒŸ ä¸­æ–‡åœç”¨è¯åˆ—è¡¨
STOP_WORDS = set(["çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€ä¸ª"])

def chinese_text_preprocess(text):
    """ğŸŒŸ ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†"""
    # å»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', text)
    # åˆ†è¯å¤„ç†
    words = jieba.cut(text)
    # å»é™¤åœç”¨è¯
    words = [w for w in words if w not in STOP_WORDS]
    return ' '.join(words)

# ğŸš€ Query Expansion with HyDE (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)
def expand_query(query, uri, model):
    try:
        # ğŸŒŸ æ·»åŠ ä¸­æ–‡æç¤ºè¯æ¨¡æ¿
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„ä¸­æ–‡å›ç­”ï¼Œç”¨äºæ”¹è¿›æ£€ç´¢æ•ˆæœã€‚ä¿æŒå›ç­”ç®€æ´ï¼Œç”¨ä¹¦é¢ä¸­æ–‡ã€‚
        
        é—®é¢˜ï¼š{query}
        å‡è®¾å›ç­”ï¼š"""
        
        response = requests.post(uri, json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "temperature": 0.7  # ğŸŒŸ è°ƒæ•´ç”Ÿæˆå¤šæ ·æ€§
        }, timeout=10).json()
        
        # ğŸŒŸ ç»“æœæ¸…æ´—
        generated = response.get('response', '').strip()
        generated = re.sub(r'\n+', ' ', generated)  # å»é™¤å¤šä½™æ¢è¡Œ
        return f"{query}\n{generated}"
    except Exception as e:
        st.error(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {str(e)}")
        return query

# ğŸš€ Advanced Retrieval Pipeline (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)
def retrieve_documents(query, uri, model, chat_history=""):
    # ğŸŒŸ ä¸­æ–‡é¢„å¤„ç†
    processed_query = chinese_text_preprocess(query)
    
    if st.session_state.enable_hyde:
        expanded_query = expand_query(f"{chat_history}\n{processed_query}", uri, model)
        expanded_query = chinese_text_preprocess(expanded_query)  # ğŸŒŸ æ‰©å±•æŸ¥è¯¢ä¹Ÿé¢„å¤„ç†
    else:
        expanded_query = processed_query

    # ğŸ” ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„æ£€ç´¢æµç¨‹
    docs = st.session_state.retrieval_pipeline["ensemble"].invoke(
        expanded_query,
        search_kwargs={"k": st.session_state.max_contexts*2}  # ğŸŒŸ æ‰©å¤§åˆå§‹æ£€ç´¢èŒƒå›´
    )
    
    # ğŸš€ GraphRAG ä¸­æ–‡ä¼˜åŒ–
    if st.session_state.enable_graph_rag:
        # ğŸŒŸ ä¸­æ–‡å›¾æŸ¥è¯¢é¢„å¤„ç†
        graph_query = chinese_text_preprocess(query)
        graph_results = retrieve_from_graph(
            graph_query, 
            st.session_state.retrieval_pipeline["knowledge_graph"],
            lang="zh"  # ğŸŒŸ æŒ‡å®šä¸­æ–‡æ¨¡å¼
        )
        
        # ğŸŒŸ å¤„ç†å›¾æ£€ç´¢ç»“æœ
        graph_docs = []
        for node in graph_results:
            if isinstance(node, dict):
                content = node.get("text_zh", "") or node.get("text", "")  # ğŸŒŸ ä¼˜å…ˆå–ä¸­æ–‡å†…å®¹
            else:
                content = str(node)
            graph_docs.append(Document(
                page_content=content,
                metadata={"source": "knowledge_graph"}
            ))
        
        if graph_docs:
            docs = graph_docs + docs

    # ğŸš€ ä¸­æ–‡é‡æ’åºä¼˜åŒ–
    if st.session_state.enable_reranking:
        # ğŸŒŸ ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„reranker
        reranker = st.session_state.retrieval_pipeline["reranker"]
        pairs = [[processed_query, chinese_text_preprocess(doc.page_content)] for doc in docs]
        
        # ğŸŒŸ æ‰¹é‡å¤„ç†æå‡æ€§èƒ½
        batch_size = 32
        scores = []
        for i in range(0, len(pairs), batch_size):
            batch = pairs[i:i+batch_size]
            scores.extend(reranker.predict(batch))
        
        # æŒ‰åˆ†æ•°æ’åº
        ranked_docs = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]
    else:
        ranked_docs = docs

    # ğŸŒŸ ç»“æœåå¤„ç†
    final_docs = []
    for doc in ranked_docs[:st.session_state.max_contexts]:
        # ğŸŒŸ ç¡®ä¿å†…å®¹ä¸ºä¸­æ–‡
        content = doc.page_content
        if not any('\u4e00' <= c <= '\u9fff' in content for c in content):
            continue  # è¿‡æ»¤éä¸­æ–‡å†…å®¹
        final_docs.append(doc)
    
    return final_docs
