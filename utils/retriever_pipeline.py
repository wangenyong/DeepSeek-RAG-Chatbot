import streamlit as st
from utils.build_graph import retrieve_from_graph
from utils.chinese_tools import chinese_text_preprocess
from langchain_core.documents import Document
import requests
import re
import logging
from datetime import datetime

# ğŸš€ Query Expansion with HyDE (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)
def expand_query(query, uri, model):
    logging.info("ğŸ” å¼€å§‹æŸ¥è¯¢æ‰©å±•å¤„ç† | åŸå§‹æŸ¥è¯¢ï¼š%s", query[:50]+"..." if len(query)>50 else query)
    start_time = datetime.now()
    
    try:
        # ğŸŒŸ æ·»åŠ ä¸­æ–‡æç¤ºè¯æ¨¡æ¿
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„ä¸­æ–‡å›ç­”ï¼Œç”¨äºæ”¹è¿›æ£€ç´¢æ•ˆæœã€‚ä¿æŒå›ç­”ç®€æ´ï¼Œç”¨ä¹¦é¢ä¸­æ–‡ã€‚
        
        é—®é¢˜ï¼š{query}
        å‡è®¾å›ç­”ï¼š"""
        
        logging.info("ç”Ÿæˆæç¤ºæ¨¡æ¿ | é•¿åº¦ï¼š%d å­—ç¬¦ | ç¤ºä¾‹ï¼š%s", 
                    len(prompt), prompt[:100].replace('\n', ' ')+"...")
        
        logging.info("è°ƒç”¨è¯­è¨€æ¨¡å‹ | æœåŠ¡åœ°å€ï¼š%s | æ¨¡å‹ï¼š%s", uri, model)
        response = requests.post(uri, json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "temperature": 0.7  # ğŸŒŸ è°ƒæ•´ç”Ÿæˆå¤šæ ·æ€§
        }, timeout=10).json()
        logging.info("æ¨¡å‹å“åº”æ¥æ”¶ | çŠ¶æ€ç ï¼š%d | å“åº”é•¿åº¦ï¼š%d", 
                     response.status_code, len(response.text))
        
        # ğŸŒŸ ç»“æœæ¸…æ´—
        generated = response.get('response', '').strip()
        logging.info("åŸå§‹ç”Ÿæˆå†…å®¹ | é•¿åº¦ï¼š%d å­—ç¬¦ | ç¤ºä¾‹ï¼š%s",
                    len(generated), generated[:50].replace('\n', ' ')+"...")
        
        # ç»“æœå¤„ç†
        cleaned = re.sub(r'\n+', ' ', generated)
        logging.info("æŸ¥è¯¢æ‰©å±•å®Œæˆ | è€—æ—¶ï¼š%.2fs | æ‰©å±•åé•¿åº¦ï¼š%dâ†’%d å­—ç¬¦",
                    (datetime.now()-start_time).total_seconds(),
                    len(query), len(cleaned))
        return f"{query}\n{cleaned}"
    except Exception as e:
        logging.error("æŸ¥è¯¢æ‰©å±•å¤±è´¥ | é”™è¯¯ç±»å‹ï¼š%s | è¯¦æƒ…ï¼š%s", 
                     type(e).__name__, str(e), exc_info=True)
        st.error(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {str(e)}")
        return query

# ğŸš€ Advanced Retrieval Pipeline (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)
def retrieve_documents(query, uri, model, chat_history=""):
    logging.info("ğŸš€ å¼€å§‹æ–‡æ¡£æ£€ç´¢æµç¨‹ | åŸå§‹æŸ¥è¯¢ï¼š%s", query[:100]+"..." if len(query)>100 else query)
    start_time = datetime.now()
    
    try:
        # ğŸŒŸ ä¸­æ–‡é¢„å¤„ç†
        processed_query = chinese_text_preprocess(query)
        logging.info("æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ | åŸå§‹é•¿åº¦ï¼š%d â†’ å¤„ç†åï¼š%d å­—ç¬¦",
                     len(query), len(processed_query))
        # HyDEæ‰©å±•
        if st.session_state.enable_hyde:
            logging.info("å¯ç”¨HyDEæ‰©å±• | å†å²ä¸Šä¸‹æ–‡é•¿åº¦ï¼š%d å­—ç¬¦", len(chat_history))
            expanded_query = expand_query(f"{chat_history}\n{processed_query}", uri, model)
            expanded_query = chinese_text_preprocess(expanded_query)  # ğŸŒŸ æ‰©å±•æŸ¥è¯¢ä¹Ÿé¢„å¤„ç†
            logging.info("æ‰©å±•åæŸ¥è¯¢ | é•¿åº¦ï¼š%d å­—ç¬¦ | ç¤ºä¾‹ï¼š%s",
                         len(expanded_query), expanded_query[:100]+"...")
        else:
            logging.info("HyDEæ‰©å±•æœªå¯ç”¨")
            expanded_query = processed_query

        # ğŸ” ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„æ£€ç´¢æµç¨‹
        # æ··åˆæ£€ç´¢
        logging.info("æ‰§è¡Œæ··åˆæ£€ç´¢ | æœ€å¤§å¬å›æ•°ï¼š%d", st.session_state.max_contexts*2)
        docs = st.session_state.retrieval_pipeline["ensemble"].invoke(
            expanded_query,
            search_kwargs={"k": st.session_state.max_contexts*2}  # ğŸŒŸ æ‰©å¤§åˆå§‹æ£€ç´¢èŒƒå›´
        )
        logging.info("åˆæ­¥å¬å›ç»“æœ | æ–‡æ¡£æ•°ï¼š%d", len(docs))
        
        # ğŸš€ GraphRAG ä¸­æ–‡ä¼˜åŒ–
        if st.session_state.enable_graph_rag:
            logging.info("å¯ç”¨GraphRAGå¢å¼º")
            # ğŸŒŸ ä¸­æ–‡å›¾æŸ¥è¯¢é¢„å¤„ç†
            graph_query = chinese_text_preprocess(query)
            graph_results = retrieve_from_graph(
                graph_query, 
                st.session_state.retrieval_pipeline["knowledge_graph"]
            )
            logging.info("å›¾è°±åŒ¹é…ç»“æœ | åŒ¹é…å®ä½“æ•°ï¼š%d", len(graph_results))
            if graph_results:
                logging.info("TOP3å›¾è°±å®ä½“ï¼š%s", graph_results[:3])
            
            # ğŸŒŸ å¤„ç†å›¾æ£€ç´¢ç»“æœ
            graph_docs = []
            for node in graph_results:
                if isinstance(node, dict):
                    content = node.get("text_zh", "") or node.get("text", "")  # ğŸŒŸ ä¼˜å…ˆå–ä¸­æ–‡å†…å®¹
                else:
                    content = str(node)
                graph_docs.append(Document(
                    page_content=content,
                    metadata={"source": "knowledge_graph"}
                ))
            
            if graph_docs:
                docs = graph_docs + docs
                logging.info("å¢å¼ºåæ–‡æ¡£æ€»æ•°ï¼š%d", len(docs))

        # ğŸš€ ä¸­æ–‡é‡æ’åºä¼˜åŒ–
        if st.session_state.enable_reranking:
            logging.info("æ‰§è¡Œé‡æ’åº | æ–‡æ¡£æ•°ï¼š%d | æ‰¹å¤„ç†å¤§å°ï¼š%d", len(docs), batch_size)
            # ğŸŒŸ ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„reranker
            reranker = st.session_state.retrieval_pipeline["reranker"]
            pairs = [[processed_query, chinese_text_preprocess(doc.page_content)] for doc in docs]
            
            # ğŸŒŸ æ‰¹é‡å¤„ç†æå‡æ€§èƒ½
            batch_size = 32
            scores = []
            logging.info("æ‰§è¡Œé‡æ’åº | æ–‡æ¡£æ•°ï¼š%d | æ‰¹å¤„ç†å¤§å°ï¼š%d", len(docs), batch_size)
            for i in range(0, len(pairs), batch_size):
                batch = pairs[i:i+batch_size]
                scores.extend(reranker.predict(batch))
                logging.info("å·²å¤„ç†æ‰¹æ¬¡ï¼š%d/%d", i//batch_size+1, len(pairs)//batch_size+1)
            logging.info("é‡æ’åºå®Œæˆ | æœ€é«˜åˆ†ï¼š%.2f | æœ€ä½åˆ†ï¼š%.2f", 
                        max(scores) if scores else 0, min(scores) if scores else 0)
            # æŒ‰åˆ†æ•°æ’åº
            ranked_docs = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]
        else:
            ranked_docs = docs

        # ğŸŒŸ ç»“æœåå¤„ç†
        original_count = len(ranked_docs)
        final_docs = []
        for doc in ranked_docs[:st.session_state.max_contexts]:
            content = doc.page_content
            # æ·»åŠ ä¸­æ–‡å†…å®¹æ¯”ä¾‹é˜ˆå€¼æ§åˆ¶
            MIN_CHINESE_RATIO = 0.3  # è‡³å°‘30%å­—ç¬¦æ˜¯ä¸­æ–‡
            chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
            if chinese_chars / len(content) < MIN_CHINESE_RATIO:
                continue
            final_docs.append(doc)
        
        logging.info("ç»“æœè¿‡æ»¤ | åŸå§‹æ•°ï¼š%d â†’ æœ€ç»ˆæ•°ï¼š%d | è¿‡æ»¤æ¯”ä¾‹ï¼š%.1f%%",
                    original_count, len(final_docs), 
                    (original_count-len(final_docs))/original_count*100 if original_count else 0)

        duration = (datetime.now()-start_time).total_seconds()
        logging.info("âœ… æ£€ç´¢æµç¨‹å®Œæˆ | æ€»è€—æ—¶ï¼š%.2fs | æœ€ç»ˆè¿”å›æ•°ï¼š%d", duration, len(final_docs))
        return final_docs
    
    except Exception as e:
        logging.error("æ£€ç´¢æµç¨‹å¼‚å¸¸ç»ˆæ­¢ | é˜¶æ®µï¼š%s", "æœªçŸ¥", exc_info=True)
        raise

