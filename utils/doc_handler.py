import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from utils.build_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import os
import jieba
from langchain_core.documents import Document
from langchain.text_splitter import SpacyTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings



def process_documents(uploaded_files, reranker, embedding_model, device):
    if st.session_state.documents_loaded:
        return

    st.session_state.processing = True
    documents = []
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ï¼ˆå¢åŠ ä¸­æ–‡è·¯å¾„æ”¯æŒï¼‰
    if not os.path.exists("temp"):
        os.makedirs("temp", exist_ok=True)
    
    # æ–‡ä»¶å¤„ç†ï¼ˆå¢åŠ ç¼–ç å¤„ç†ï¼‰
    for file in uploaded_files:
        try:
            file_path = os.path.join("temp", file.name)
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())

            # å¢åŠ ä¸­æ–‡PDFæ”¯æŒï¼ˆè§£å†³PDFè§£æä¹±ç ï¼‰
            if file.name.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
                loader._pdf_parser.text = lambda x: x.get_text().encode('utf-8', errors='replace').decode('utf-8')
            elif file.name.endswith(".docx"):
                loader = Docx2txtLoader(file_path)
            elif file.name.endswith(".txt"):
                with open(file_path, "r", encoding='utf-8', errors='replace') as f:
                    text = f.read()
                documents.append(Document(page_content=text))
                continue
            else:
                continue
                
            documents.extend(loader.load())
            os.remove(file_path)
        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file.name}: {str(e)}")
            return

    # ä¸­æ–‡æ–‡æœ¬åˆ†å‰²
    text_splitter = SpacyTextSplitter(
        pipeline="zh_core_web_sm",
        chunk_size=400,
        chunk_overlap=80,
        separator="\n"
    )
    texts = text_splitter.split_documents(documents)
    text_contents = [doc.page_content for doc in texts]

    # ğŸš€ ä¸­æ–‡åµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model,
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # å‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(texts, embeddings)
    
    # BM25ä¸­æ–‡æ£€ç´¢
    bm25_retriever = BM25Retriever.from_texts(
        text_contents, 
        bm25_impl=BM25Okapi,
        preprocess_func=lambda text: [word for word in jieba.lcut(text) if word.strip()]
    )

    # æ··åˆæ£€ç´¢ï¼ˆè°ƒæ•´æƒé‡ï¼‰
    ensemble_retriever = EnsembleRetriever(
        retrievers=[
            bm25_retriever,
            vector_store.as_retriever(search_kwargs={"k": 8})  # å¢åŠ å¬å›æ•°é‡
        ],
        weights=[0.3, 0.7]  # æé«˜å‘é‡æ£€ç´¢æƒé‡
    )

    # å­˜å‚¨ä¼šè¯çŠ¶æ€
    st.session_state.retrieval_pipeline = {
        "ensemble": ensemble_retriever,
        "reranker": reranker,
        "texts": text_contents,
        "knowledge_graph": build_knowledge_graph(texts)
    }

    st.session_state.documents_loaded = True
    st.session_state.processing = False

    # âœ… è°ƒè¯•ä¿¡æ¯ä¸­æ–‡åŒ–
    if "knowledge_graph" in st.session_state.retrieval_pipeline:
        G = st.session_state.retrieval_pipeline["knowledge_graph"]
        st.write(f"ğŸ”— æ€»èŠ‚ç‚¹æ•°: {len(G.nodes)}")
        st.write(f"ğŸ”— æ€»è¾¹æ•°: {len(G.edges)}")
        st.write(f"ğŸ”— ç¤ºä¾‹èŠ‚ç‚¹: {list(G.nodes)[:10]}")
        st.write(f"ğŸ”— ç¤ºä¾‹å…³ç³»: {list(G.edges(data=True))[:5]}")
