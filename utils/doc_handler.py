import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from utils.build_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import jieba
from pathlib import Path
from langchain_core.documents import Document
from langchain.text_splitter import SpacyTextSplitter, RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
import pdfplumber  # æ›´å¯é çš„PDFè§£æåº“
from langchain_community.document_loaders import Docx2txtLoader, TextLoader
import tempfile
import spacy


# é…ç½®å¸¸é‡
SUPPORTED_EXT = ['.pdf', '.docx', '.txt']
TEMP_DIR = Path("temp")
TEMP_DIR.mkdir(exist_ok=True, parents=True)  # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨

def process_uploaded_files(uploaded_files):
    """ç»Ÿä¸€å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„ä¸»å‡½æ•°"""
    documents = []
    
    for file in uploaded_files:
        file_name = file.name
        file_ext = Path(file_name).suffix.lower()
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å®‰å…¨ä¸Šä¸‹æ–‡
        with tempfile.NamedTemporaryFile(
            dir=TEMP_DIR,
            suffix=file_ext,
            delete=False  # æ‰‹åŠ¨æ§åˆ¶åˆ é™¤
        ) as temp_file:
            try:
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                temp_file.write(file.getbuffer())
                temp_path = Path(temp_file.name)
                
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†å™¨
                if file_ext == '.pdf':
                    # ä½¿ç”¨pdfplumberæå–æ–‡æœ¬ï¼ˆæ›´å¥½çš„ä¸­æ–‡æ”¯æŒï¼‰
                    with pdfplumber.open(temp_path) as pdf:
                        text = "\n\n".join(
                            f"Page {i+1}:\n{p.extract_text()}" 
                            for i, p in enumerate(pdf.pages)
                        )
                    documents.append(Document(page_content=text))
                    
                elif file_ext == '.docx':
                    loader = Docx2txtLoader(str(temp_path))
                    documents.extend(loader.load())
                    
                elif file_ext == '.txt':
                    # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹ç¼–ç çš„åŠ è½½å™¨
                    loader = TextLoader(
                        str(temp_path),
                        autodetect_encoding=True
                    )
                    documents.extend(loader.load())
                    
                else:
                    st.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name}")
                    continue
                    
            except pdfplumber.PDFSyntaxError as e:
                st.error(f"PDFè§£æå¤±è´¥ [{file_name}]: æ–‡ä»¶å¯èƒ½å·²æŸå")
                continue
            except UnicodeDecodeError as e:
                st.error(f"ç¼–ç è§£æå¤±è´¥ [{file_name}]: è¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç æ ¼å¼")
                continue
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶ [{file_name}] å¤±è´¥: {str(e)}")
                continue
            finally:
                # ç¡®ä¿åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                try:
                    temp_path.unlink()
                except Exception as e:
                    st.error(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {str(e)}")

    return documents

def chinese_text_split(documents):
    """å¢å¼ºå‹ä¸­æ–‡æ–‡æœ¬åˆ†å‰²"""
    try:
        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨Spacyè¯­ä¹‰åˆ†å‰²
        nlp = spacy.load("zh_core_web_sm")
        text_splitter = SpacyTextSplitter(
            pipeline="zh_core_web_sm",
            chunk_size=100,
            chunk_overlap=20,
            separator=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ"]  # å¤šçº§åˆ†éš”ç¬¦
        )
    except:
        # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°é€’å½’å­—ç¬¦åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=20,
            separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ", "\n", "ï¼Œ", ""]
        )
    
    texts = text_splitter.split_documents(documents)
    
    # åå¤„ç†
    text_contents = [
        doc.page_content.strip() 
        for doc in texts
        if len(doc.page_content.strip()) > 20  # è¿‡æ»¤ç©ºå†…å®¹
    ]
    
    return (texts, text_contents)

def process_documents(uploaded_files, reranker, embedding_model, device):
    if st.session_state.documents_loaded:
        return

    st.session_state.processing = True
    documents = process_uploaded_files(uploaded_files=uploaded_files)
    
    texts, text_contents = chinese_text_split(documents)

    # ğŸš€ ä¸­æ–‡åµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model,
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # å‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(texts, embeddings)
    
    # BM25ä¸­æ–‡æ£€ç´¢
    bm25_retriever = BM25Retriever.from_texts(
        text_contents, 
        bm25_impl=BM25Okapi,
        preprocess_func=lambda text: [word for word in jieba.lcut(text) if word.strip()]
    )

    # æ··åˆæ£€ç´¢ï¼ˆè°ƒæ•´æƒé‡ï¼‰
    ensemble_retriever = EnsembleRetriever(
        retrievers=[
            bm25_retriever,
            vector_store.as_retriever(search_kwargs={"k": 8})  # å¢åŠ å¬å›æ•°é‡
        ],
        weights=[0.3, 0.7]  # æé«˜å‘é‡æ£€ç´¢æƒé‡
    )

    # å­˜å‚¨ä¼šè¯çŠ¶æ€
    st.session_state.retrieval_pipeline = {
        "ensemble": ensemble_retriever,
        "reranker": reranker,
        "texts": text_contents,
        "knowledge_graph": build_knowledge_graph(texts)
    }

    st.session_state.documents_loaded = True
    st.session_state.processing = False

    # âœ… è°ƒè¯•ä¿¡æ¯ä¸­æ–‡åŒ–
    if "knowledge_graph" in st.session_state.retrieval_pipeline:
        G = st.session_state.retrieval_pipeline["knowledge_graph"]
        st.write(f"ğŸ”— æ€»èŠ‚ç‚¹æ•°: {len(G.nodes)}")
        st.write(f"ğŸ”— æ€»è¾¹æ•°: {len(G.edges)}")
        st.write(f"ğŸ”— ç¤ºä¾‹èŠ‚ç‚¹: {list(G.nodes)[:10]}")
        st.write(f"ğŸ”— ç¤ºä¾‹å…³ç³»: {list(G.edges(data=True))[:5]}")
