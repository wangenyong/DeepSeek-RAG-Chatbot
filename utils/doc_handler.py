import streamlit as st
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from utils.build_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import jieba
from pathlib import Path
from langchain_core.documents import Document
from langchain.text_splitter import SpacyTextSplitter, RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
import pdfplumber  # æ›´å¯é çš„PDFè§£æåº“
from langchain_community.document_loaders import Docx2txtLoader, TextLoader
import tempfile
import spacy
import re
import logging


# é…ç½®å¸¸é‡
SUPPORTED_EXT = ['.pdf', '.docx', '.txt']
TEMP_DIR = Path("temp")
TEMP_DIR.mkdir(exist_ok=True, parents=True)  # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨

def process_uploaded_files(uploaded_files):
    """ç»Ÿä¸€å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„ä¸»å‡½æ•°"""
    documents = []
    
    for file in uploaded_files:
        file_name = file.name
        file_ext = Path(file_name).suffix.lower()
        logging.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name} | ç±»å‹ï¼š{file_ext}")

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å®‰å…¨ä¸Šä¸‹æ–‡
        with tempfile.NamedTemporaryFile(
            dir=TEMP_DIR,
            suffix=file_ext,
            delete=False
        ) as temp_file:
            try:
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                temp_file.write(file.getbuffer())
                temp_path = Path(temp_file.name)
                logging.info(f"åˆ›å»ºä¸´æ—¶æ–‡ä»¶æˆåŠŸ | è·¯å¾„ï¼š{temp_path}")

                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†å™¨
                if file_ext == '.pdf':
                    logging.info(f"å¼€å§‹è§£æPDFæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    with pdfplumber.open(temp_path) as pdf:
                        text = "\n\n".join(
                            f"Page {i+1}:\n{p.extract_text()}" 
                            for i, p in enumerate(pdf.pages)
                        )
                        logging.info(f"PDFè§£æå®Œæˆ | é¡µæ•°ï¼š{len(pdf.pages)} | å­—ç¬¦æ•°ï¼š{len(text)}")
                    documents.append(Document(page_content=text))
                    
                elif file_ext == '.docx':
                    logging.info(f"å¼€å§‹è§£æDOCXæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    loader = Docx2txtLoader(str(temp_path))
                    docs = loader.load()
                    logging.info(f"DOCXè§£æå®Œæˆ | æ®µè½æ•°ï¼š{len(docs)}")
                    documents.extend(docs)
                    
                elif file_ext == '.txt':
                    logging.info(f"å¼€å§‹è§£æTXTæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    loader = TextLoader(
                        str(temp_path),
                        autodetect_encoding=True
                    )
                    docs = loader.load()
                    logging.info(f"TXTè§£æå®Œæˆ | å­—ç¬¦æ•°ï¼š{len(docs[0].page_content) if docs else 0}")
                    documents.extend(docs)
                    
                # æ–°å¢Excelå¤„ç†åˆ†æ”¯
                elif file_ext in ('.xls', '.xlsx'):
                    logging.info(f"å¼€å§‹è§£æExcelæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    
                    # è¯»å–Excelæ–‡ä»¶
                    df_dict = pd.read_excel(temp_path, sheet_name=None)
                    text_content = []
                    
                    # éå†æ‰€æœ‰å·¥ä½œè¡¨
                    for sheet_name, df in df_dict.items():
                        sheet_text = [
                            f"å·¥ä½œè¡¨ã€{sheet_name}ã€‘",
                            "è¡¨å¤´ï¼š" + ", ".join(df.columns.astype(str)),
                            "æ•°æ®ï¼š\n" + df.to_string(index=False)
                        ]
                        text_content.append("\n".join(sheet_text))
                    
                    full_text = "\n\n".join(text_content)
                    documents.append(Document(page_content=full_text))
                    logging.info(f"Excelè§£æå®Œæˆ | å·¥ä½œè¡¨æ•°ï¼š{len(df_dict)} | æ€»è¡Œæ•°ï¼š{sum(len(df) for df in df_dict.values())}")
                    
                else:
                    logging.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ | æ–‡ä»¶åï¼š{file_name}")
                    continue
                    
            except pdfplumber.PDFSyntaxError as e:
                logging.error(f"PDFè§£æå¤±è´¥ | æ–‡ä»¶åï¼š{file_name}", exc_info=True)
                continue
            except UnicodeDecodeError as e:
                logging.error(f"ç¼–ç è§£æå¤±è´¥ | æ–‡ä»¶åï¼š{file_name} | é”™è¯¯ï¼š{str(e)}")
                continue
            except Exception as e:
                logging.error(f"æ–‡ä»¶å¤„ç†å¼‚å¸¸ | æ–‡ä»¶åï¼š{file_name}", exc_info=True)
                continue
            finally:
                try:
                    if temp_path.exists():
                        temp_path.unlink()
                        logging.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç† | è·¯å¾„ï¼š{temp_path}")
                except Exception as e:
                    logging.error(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥ | è·¯å¾„ï¼š{temp_path}", exc_info=True)

    logging.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ | æ€»å¤„ç†æ–‡ä»¶æ•°ï¼š{len(uploaded_files)} | æœ‰æ•ˆæ–‡æ¡£æ•°ï¼š{len(documents)}")
    return documents

def chinese_text_split(documents):
    logging.info("å¼€å§‹æ–‡æœ¬åˆ†å‰² | åŸå§‹æ–‡æ¡£æ•°ï¼š%d", len(documents))
    
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ"],  # æŒ‰æ®µè½ã€å¥å­åˆ†å‰²
        chunk_size=800,
        chunk_overlap=160,
        is_separator_regex=False  # æ˜ç¡®å…³é—­æ­£åˆ™åŒ¹é…ï¼Œç›´æ¥æŒ‰å­—ç¬¦åŒ¹é…
    )
    
    texts = text_splitter.split_documents(documents)
    logging.info("åˆæ­¥åˆ†å‰²å®Œæˆ | å—æ•°ï¼š%d", len(texts))
    
    # åå¤„ç†
    text_contents = [
        doc.page_content.strip() 
        for doc in texts
        if len(doc.page_content.strip()) > 20  # è¿‡æ»¤ç©ºå†…å®¹
    ]
    
    return (texts, text_contents)

def process_documents(uploaded_files, reranker, embedding_model, device):
    if st.session_state.documents_loaded:
        logging.info("æ–‡æ¡£å·²åŠ è½½ï¼Œè·³è¿‡å¤„ç†æµç¨‹")
        return

    try:
        st.session_state.processing = True
        logging.info("ğŸ“‚ å¼€å§‹æ–‡æ¡£å¤„ç†æµç¨‹")
        
        # æ–‡ä»¶å¤„ç†
        logging.info(f"æ”¶åˆ°ä¸Šä¼ æ–‡ä»¶ | æ•°é‡ï¼š{len(uploaded_files)}")
        documents = process_uploaded_files(uploaded_files=uploaded_files)
        logging.info(f"åŸå§‹æ–‡æ¡£å¤„ç†å®Œæˆ | æœ‰æ•ˆæ–‡æ¡£æ•°ï¼š{len(documents)}")
        
        # æ–‡æœ¬åˆ†å‰²
        logging.info("å¼€å§‹ä¸­æ–‡æ–‡æœ¬åˆ†å‰²")
        texts, text_contents = chinese_text_split(documents)
        logging.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆ | æ€»æ®µè½æ•°ï¼š{len(texts)} | å¹³å‡é•¿åº¦ï¼š{sum(len(t) for t in text_contents)//len(text_contents)}å­—ç¬¦")

        # åµŒå…¥æ¨¡å‹åˆå§‹åŒ–
        logging.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ | æ¨¡å‹ï¼š{embedding_model} | è®¾å¤‡ï¼š{device}")
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # å‘é‡å­˜å‚¨
        logging.info("åˆ›å»ºFAISSå‘é‡å­˜å‚¨")
        vector_store = FAISS.from_documents(texts, embeddings)
        logging.info(f"å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ | æ–‡æ¡£æ•°ï¼š{vector_store.index.ntotal} | ç»´åº¦ï¼š{vector_store.index.d}")

        # BM25æ£€ç´¢å™¨
        logging.info("åˆå§‹åŒ–BM25æ£€ç´¢å™¨")
        bm25_retriever = BM25Retriever.from_texts(
            text_contents, 
            bm25_impl=BM25Okapi,
            preprocess_func=lambda text: [word for word in jieba.lcut(text) if word.strip()]
        )
        logging.info(f"BM25æ£€ç´¢å™¨å°±ç»ª | æ–‡æ¡£æ•°ï¼š{len(text_contents)}")

        # æ··åˆæ£€ç´¢å™¨
        logging.info("é…ç½®æ··åˆæ£€ç´¢å™¨")
        ensemble_retriever = EnsembleRetriever(
            retrievers=[
                bm25_retriever,
                vector_store.as_retriever(search_kwargs={"k": 8})
            ],
            weights=[0.3, 0.7]
        )
        logging.info(f"æ··åˆæ£€ç´¢å™¨é…ç½®å®Œæˆ | æƒé‡ï¼šBM25(30%) + å‘é‡(70%) | å¬å›æ•°é‡ï¼š8")

        # çŸ¥è¯†å›¾è°±æ„å»º
        # logging.info("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
        # knowledge_graph = build_knowledge_graph(texts)
        # logging.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ | èŠ‚ç‚¹æ•°ï¼š{len(knowledge_graph.nodes)} | è¾¹æ•°ï¼š{len(knowledge_graph.edges)}")

        # å­˜å‚¨ä¼šè¯çŠ¶æ€
        st.session_state.retrieval_pipeline = {
            "ensemble": ensemble_retriever,
            "reranker": reranker,
            "texts": text_contents,
        }
        logging.info("æ£€ç´¢ç®¡é“é…ç½®å®Œæˆ")

        st.session_state.documents_loaded = True
        logging.info("âœ… æ–‡æ¡£å¤„ç†æµç¨‹å®Œæˆ")

    except Exception as e:
        logging.error("æ–‡æ¡£å¤„ç†æµç¨‹å¼‚å¸¸ç»ˆæ­¢", exc_info=True)
        st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    finally:
        st.session_state.processing = False
        logging.info("æ¸…ç†å¤„ç†çŠ¶æ€")

    # çŸ¥è¯†å›¾è°±è°ƒè¯•ä¿¡æ¯
    if "knowledge_graph" in st.session_state.retrieval_pipeline:
        try:
            G = st.session_state.retrieval_pipeline["knowledge_graph"]
            logging.info(f"çŸ¥è¯†å›¾è°±ç»Ÿè®¡ | èŠ‚ç‚¹ç¤ºä¾‹ï¼š{list(G.nodes)[:5]}... | è¾¹ç¤ºä¾‹ï¼š{list(G.edges(data=True))[:3]}...")
            
            logging.info(f"ğŸ”— æ€»èŠ‚ç‚¹æ•°: {len(G.nodes)}")
            logging.info(f"ğŸ”— æ€»è¾¹æ•°: {len(G.edges)}")
            logging.info(f"ğŸ”— ç¤ºä¾‹èŠ‚ç‚¹: {list(G.nodes)[:10]}")
            logging.info(f"ğŸ”— ç¤ºä¾‹å…³ç³»: {list(G.edges(data=True))[:5]}")
        except Exception as e:
            logging.warning("çŸ¥è¯†å›¾è°±è°ƒè¯•ä¿¡æ¯æ˜¾ç¤ºå¤±è´¥", exc_info=True)
