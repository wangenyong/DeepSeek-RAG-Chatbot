import streamlit as st
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from utils.build_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import jieba
from pathlib import Path
from langchain_core.documents import Document
from langchain.text_splitter import SpacyTextSplitter, RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
import pdfplumber  # æ›´å¯é çš„PDFè§£æåº“
from langchain_community.document_loaders import Docx2txtLoader, TextLoader
import tempfile
# åœ¨é¡¶éƒ¨å¯¼å…¥æ–°å¢
from pandas.api.types import is_numeric_dtype
from tabulate import tabulate
import logging
import re

# é…ç½®å¸¸é‡
SUPPORTED_EXT = ['.pdf', '.docx', '.txt', '.xls', '.xlsx']
TEMP_DIR = Path("temp")
TEMP_DIR.mkdir(exist_ok=True, parents=True)  # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨

def process_excel_sheet(sheet_name: str, df: pd.DataFrame) -> Document:
    """ä¼˜åŒ–å•ä¸ªå·¥ä½œè¡¨çš„å¤„ç†é€»è¾‘"""
    # å…ƒæ•°æ®å¢å¼º
    metadata = {
        "source_type": "excel",
        "sheet_name": sheet_name,
        "columns": df.columns.astype(str).tolist(),
        "dtypes": {col: str(df[col].dtype) for col in df.columns},
        "shape": f"{len(df)}x{len(df.columns)}"
    }
    
    # è¡¨æ ¼å†…å®¹ä¼˜åŒ–å¤„ç†
    table_str = tabulate(
        df.head(1000),  # é™åˆ¶æœ€å¤§è¡Œæ•°é¿å…å†…å­˜é—®é¢˜
        headers='keys',
        tablefmt='pipe',
        showindex=False,
        maxcolwidths=30,
        stralign="left",
        numalign="center"
    )
    
    # ç»“æ„åŒ–æ–‡æ¡£æ ¼å¼
    content = f"""
# EXCELå·¥ä½œè¡¨ [{sheet_name}]

## å…ƒæ•°æ®
- åˆ—æ•°: {len(df.columns)}
- è¡Œæ•°: {len(df)}
- åˆ—ç±»å‹: {metadata['dtypes']}

## æ•°æ®é¢„è§ˆ
{table_str}
"""
    return Document(page_content=content.strip(), metadata=metadata)

def process_uploaded_files(uploaded_files):
    """ç»Ÿä¸€å¤„ç†ä¸Šä¼ æ–‡ä»¶çš„ä¸»å‡½æ•°"""
    documents = []
    
    for file in uploaded_files:
        file_name = file.name
        file_ext = Path(file_name).suffix.lower()
        logging.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name} | ç±»å‹ï¼š{file_ext}")

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å®‰å…¨ä¸Šä¸‹æ–‡
        with tempfile.NamedTemporaryFile(
            dir=TEMP_DIR,
            suffix=file_ext,
            delete=False
        ) as temp_file:
            try:
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                temp_file.write(file.getbuffer())
                temp_path = Path(temp_file.name)
                logging.info(f"åˆ›å»ºä¸´æ—¶æ–‡ä»¶æˆåŠŸ | è·¯å¾„ï¼š{temp_path}")

                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†å™¨
                if file_ext == '.pdf':
                    logging.info(f"å¼€å§‹è§£æPDFæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    with pdfplumber.open(temp_path) as pdf:
                        text = "\n\n".join(
                            f"Page {i+1}:\n{p.extract_text()}" 
                            for i, p in enumerate(pdf.pages)
                        )
                        logging.info(f"PDFè§£æå®Œæˆ | é¡µæ•°ï¼š{len(pdf.pages)} | å­—ç¬¦æ•°ï¼š{len(text)}")
                    documents.append(Document(page_content=text))
                    
                elif file_ext == '.docx':
                    logging.info(f"å¼€å§‹è§£æDOCXæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    loader = Docx2txtLoader(str(temp_path))
                    docs = loader.load()
                    logging.info(f"DOCXè§£æå®Œæˆ | æ®µè½æ•°ï¼š{len(docs)}")
                    documents.extend(docs)
                    
                elif file_ext == '.txt':
                    logging.info(f"å¼€å§‹è§£æTXTæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    loader = TextLoader(
                        str(temp_path),
                        autodetect_encoding=True
                    )
                    docs = loader.load()
                    logging.info(f"TXTè§£æå®Œæˆ | å­—ç¬¦æ•°ï¼š{len(docs[0].page_content) if docs else 0}")
                    documents.extend(docs)
                    
                # æ–°å¢Excelå¤„ç†åˆ†æ”¯
                elif file_ext in ('.xls', '.xlsx'):
                    logging.info(f"å¼€å§‹è§£æExcelæ–‡ä»¶ | æ–‡ä»¶åï¼š{file_name}")
                    try:
                        # å¢å¼ºçš„Excelè¯»å–å‚æ•°
                        df_dict = pd.read_excel(
                            temp_path,
                            sheet_name=None,
                            dtype=str,  # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²é¿å…ç±»å‹é—®é¢˜
                            na_filter=False,  # ç¦ç”¨è‡ªåŠ¨ç©ºå€¼è¿‡æ»¤
                            engine='openpyxl' if file_ext == '.xlsx' else None
                        )
                        
                        # å¤„ç†æ¯ä¸ªå·¥ä½œè¡¨
                        sheet_docs = []
                        for sheet_name, df in df_dict.items():
                            logging.debug(f"å·¥ä½œè¡¨ã€{sheet_name}ã€‘æ ·ä¾‹æ•°æ®ï¼š\n{df.head(3).to_markdown()}")
                            try:
                                # æ¸…ç†åˆ—åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                                df.columns = [re.sub(r'[\\/:*?"<>|]', '_', str(col)) for col in df.columns]
                                
                                # æ•°å€¼åˆ—ç‰¹æ®Šå¤„ç†
                                num_cols = [col for col in df.columns if is_numeric_dtype(df[col])]
                                if num_cols:
                                    df[num_cols] = df[num_cols].apply(lambda x: x.round(6))
                                
                                # ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£
                                doc = process_excel_sheet(sheet_name, df)
                                sheet_docs.append(doc)
                            except Exception as e:
                                logging.error(f"å·¥ä½œè¡¨å¤„ç†å¤±è´¥ | æ–‡ä»¶ï¼š{file_name} | å·¥ä½œè¡¨ï¼š{sheet_name}", exc_info=True)
                                continue
                        
                        documents.extend(sheet_docs)
                        logging.info(f"Excelè§£æå®Œæˆ | å·¥ä½œè¡¨æ•°ï¼š{len(sheet_docs)} | æ€»è¡Œæ•°ï¼š{sum(len(df) for df in df_dict.values())}")
                        
                    except Exception as e:
                        logging.error(f"Excelæ–‡ä»¶è§£æå¤±è´¥ | æ–‡ä»¶åï¼š{file_name}", exc_info=True)
                        continue
                    
                else:
                    logging.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ | æ–‡ä»¶åï¼š{file_name}")
                    continue
                    
            except pdfplumber.PDFSyntaxError as e:
                logging.error(f"PDFè§£æå¤±è´¥ | æ–‡ä»¶åï¼š{file_name}", exc_info=True)
                continue
            except UnicodeDecodeError as e:
                logging.error(f"ç¼–ç è§£æå¤±è´¥ | æ–‡ä»¶åï¼š{file_name} | é”™è¯¯ï¼š{str(e)}")
                continue
            except Exception as e:
                logging.error(f"æ–‡ä»¶å¤„ç†å¼‚å¸¸ | æ–‡ä»¶åï¼š{file_name}", exc_info=True)
                continue
            finally:
                try:
                    if temp_path.exists():
                        temp_path.unlink()
                        logging.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç† | è·¯å¾„ï¼š{temp_path}")
                except Exception as e:
                    logging.error(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥ | è·¯å¾„ï¼š{temp_path}", exc_info=True)

    logging.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ | æ€»å¤„ç†æ–‡ä»¶æ•°ï¼š{len(uploaded_files)} | æœ‰æ•ˆæ–‡æ¡£æ•°ï¼š{len(documents)}")
    return documents


def table_aware_splitter(documents: list) -> tuple:
    """ä¼˜åŒ–åçš„è¡¨æ ¼åˆ†å—é€»è¾‘"""
    # åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°
    table_docs = [doc for doc in documents if doc.metadata.get('source_type') == 'excel']
    avg_cols = sum(len(doc.metadata['columns']) for doc in table_docs) / (len(table_docs) or 1)
    
    chunk_size = max(800, int(avg_cols * 150))  # æ¯åˆ—çº¦150å­—ç¬¦
    chunk_overlap = min(160, int(chunk_size * 0.3))  # æé«˜é‡å æ¯”ä¾‹

    # è¡¨æ ¼ä¸“ç”¨åˆ†å‰²å™¨
    table_splitter = RecursiveCharacterTextSplitter(
        separators=[
            r'\n## æ•°æ®é¢„è§ˆ\n',  # ä¿ç•™å®Œæ•´è¡¨æ ¼
            r'\n# EXCELå·¥ä½œè¡¨ $$(.*?)$$'  # æŒ‰å·¥ä½œè¡¨åˆ†å‰²
        ],
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        is_separator_regex=True  # å¯ç”¨æ­£åˆ™è¡¨è¾¾å¼
    )
    
    # åˆ†å—åæ·»åŠ è¡¨æ ¼æ ‡è¯†
    split_docs = []
    for doc in table_docs:
        for chunk in table_splitter.split_documents([doc]):
            chunk.metadata['is_table'] = True
            split_docs.append(chunk)
    
    return split_docs, [doc.page_content for doc in split_docs]


def chinese_text_split(documents):
    logging.info("å¼€å§‹æ–‡æœ¬åˆ†å‰² | åŸå§‹æ–‡æ¡£æ•°ï¼š%d", len(documents))
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«è¡¨æ ¼æ–‡æ¡£
    if any(doc.metadata.get('source_type') == 'excel' for doc in documents):
        return table_aware_splitter(documents)
    
    # åŸå§‹åˆ†å‰²é€»è¾‘
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ"],
        chunk_size=800,
        chunk_overlap=160,
        is_separator_regex=False
    )
    
    texts = text_splitter.split_documents(documents)
    text_contents = [
        doc.page_content.strip() 
        for doc in texts
        if len(doc.page_content.strip()) > 20
    ]
    return (texts, text_contents)


# ä¿®æ”¹BM25åˆå§‹åŒ–éƒ¨åˆ†
def table_tokenizer(text: str) -> list:
    """è¡¨æ ¼æ•æ„Ÿå‹åˆ†è¯å™¨"""
    # ä¿ç•™æ•°å­—ã€è´§å¸ç¬¦å·ã€ç™¾åˆ†æ¯”ç­‰
    tokens = re.findall(r'\d+\.?\d*|[$Â¥â‚¬%]|\w+[\u4e00-\u9fff]', text)
    return [t.lower() for t in tokens if t.strip()]


def process_documents(uploaded_files, reranker, embedding_model, device):
    if st.session_state.documents_loaded:
        logging.info("æ–‡æ¡£å·²åŠ è½½ï¼Œè·³è¿‡å¤„ç†æµç¨‹")
        return

    try:
        st.session_state.processing = True
        logging.info("ğŸ“‚ å¼€å§‹æ–‡æ¡£å¤„ç†æµç¨‹")
        
        # æ–‡ä»¶å¤„ç†
        logging.info(f"æ”¶åˆ°ä¸Šä¼ æ–‡ä»¶ | æ•°é‡ï¼š{len(uploaded_files)}")
        documents = process_uploaded_files(uploaded_files=uploaded_files)
        logging.info(f"åŸå§‹æ–‡æ¡£å¤„ç†å®Œæˆ | æœ‰æ•ˆæ–‡æ¡£æ•°ï¼š{len(documents)}")
        
        # æ–‡æœ¬åˆ†å‰²
        logging.info("å¼€å§‹ä¸­æ–‡æ–‡æœ¬åˆ†å‰²")
        texts, text_contents = chinese_text_split(documents)
        logging.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆ | æ€»æ®µè½æ•°ï¼š{len(texts)} | å¹³å‡é•¿åº¦ï¼š{sum(len(t) for t in text_contents)//len(text_contents)}å­—ç¬¦")

        # åµŒå…¥æ¨¡å‹åˆå§‹åŒ–
        logging.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ | æ¨¡å‹ï¼š{embedding_model} | è®¾å¤‡ï¼š{device}")
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # å‘é‡å­˜å‚¨
        logging.info("åˆ›å»ºFAISSå‘é‡å­˜å‚¨")
        vector_store = FAISS.from_documents(texts, embeddings)
        logging.info(f"å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ | æ–‡æ¡£æ•°ï¼š{vector_store.index.ntotal} | ç»´åº¦ï¼š{vector_store.index.d}")

        # BM25æ£€ç´¢å™¨
        logging.info("åˆå§‹åŒ–BM25æ£€ç´¢å™¨")
        bm25_retriever = BM25Retriever.from_texts(
            text_contents,
            bm25_impl=BM25Okapi,
            preprocess_func=table_tokenizer  # ä½¿ç”¨å®šåˆ¶åˆ†è¯å™¨
        )
        logging.info(f"BM25æ£€ç´¢å™¨å°±ç»ª | æ–‡æ¡£æ•°ï¼š{len(text_contents)}")

        # æ··åˆæ£€ç´¢å™¨
        logging.info("é…ç½®æ··åˆæ£€ç´¢å™¨")
        # åœ¨process_documentsä¸­ä¿®æ”¹æ··åˆæ£€ç´¢å™¨é…ç½®
        ensemble_retriever = EnsembleRetriever(
            retrievers=[
                bm25_retriever,
                vector_store.as_retriever(search_kwargs={
                    "k": 12,  # å¢åŠ å‘é‡å¬å›é‡
                    "score_threshold": 0.65  # é™ä½é˜ˆå€¼
                })
            ],
            weights=[0.5, 0.5]  # å¹³è¡¡æƒé‡
        )
        logging.info(f"æ··åˆæ£€ç´¢å™¨é…ç½®å®Œæˆ | æƒé‡ï¼šBM25(30%) + å‘é‡(70%) | å¬å›æ•°é‡ï¼š8")

        # çŸ¥è¯†å›¾è°±æ„å»º
        # logging.info("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
        # knowledge_graph = build_knowledge_graph(texts)
        # logging.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ | èŠ‚ç‚¹æ•°ï¼š{len(knowledge_graph.nodes)} | è¾¹æ•°ï¼š{len(knowledge_graph.edges)}")

        # å­˜å‚¨ä¼šè¯çŠ¶æ€
        st.session_state.retrieval_pipeline = {
            "ensemble": ensemble_retriever,
            "reranker": reranker,
            "texts": text_contents,
        }
        logging.info("æ£€ç´¢ç®¡é“é…ç½®å®Œæˆ")

        st.session_state.documents_loaded = True
        logging.info("âœ… æ–‡æ¡£å¤„ç†æµç¨‹å®Œæˆ")

    except Exception as e:
        logging.error("æ–‡æ¡£å¤„ç†æµç¨‹å¼‚å¸¸ç»ˆæ­¢", exc_info=True)
        st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    finally:
        st.session_state.processing = False
        logging.info("æ¸…ç†å¤„ç†çŠ¶æ€")

    # çŸ¥è¯†å›¾è°±è°ƒè¯•ä¿¡æ¯
    if "knowledge_graph" in st.session_state.retrieval_pipeline:
        try:
            G = st.session_state.retrieval_pipeline["knowledge_graph"]
            logging.info(f"çŸ¥è¯†å›¾è°±ç»Ÿè®¡ | èŠ‚ç‚¹ç¤ºä¾‹ï¼š{list(G.nodes)[:5]}... | è¾¹ç¤ºä¾‹ï¼š{list(G.edges(data=True))[:3]}...")
            
            logging.info(f"ğŸ”— æ€»èŠ‚ç‚¹æ•°: {len(G.nodes)}")
            logging.info(f"ğŸ”— æ€»è¾¹æ•°: {len(G.edges)}")
            logging.info(f"ğŸ”— ç¤ºä¾‹èŠ‚ç‚¹: {list(G.nodes)[:10]}")
            logging.info(f"ğŸ”— ç¤ºä¾‹å…³ç³»: {list(G.edges(data=True))[:5]}")
        except Exception as e:
            logging.warning("çŸ¥è¯†å›¾è°±è°ƒè¯•ä¿¡æ¯æ˜¾ç¤ºå¤±è´¥", exc_info=True)
